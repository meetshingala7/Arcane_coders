{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "UoPYPem0fy0I",
        "outputId": "60b90914-665a-4fe4-fdaa-f5ccf864b751"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a88c8abb2991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.feature_extractor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from utils.feature_extractor import featureExtractor\n",
        "from utils.data_loader import TrainDataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from utils.MLP import MLP\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def extract_feature_data(dataset_dir):\n",
        "    img_list = os.listdir(dataset_dir)\n",
        "    num_images = len(img_list)\n",
        "    feature_extractor = featureExtractor()\n",
        "    all_features = []\n",
        "    for ind, image_name in enumerate(img_list):\n",
        "        print(\"Feature-Extraction: %d / %d images processed..\" % (ind, num_images))\n",
        "        if(ind % 2 == 0):\n",
        "            continue\n",
        "        # Read the image\n",
        "        img = cv2.imread(os.path.join(dataset_dir, image_name), 0)\n",
        "\n",
        "        # Resize the image by the downsampling factor\n",
        "        feature_extractor.resize_image(img, np.shape(img)[0], np.shape(img)[1])\n",
        "\n",
        "        # compute the image ROI using local entropy filter\n",
        "        feature_extractor.compute_roi()\n",
        "\n",
        "        # extract the blur features using DCT transform coefficients\n",
        "        extracted_features = feature_extractor.extract_feature()\n",
        "\n",
        "        all_features.append(extracted_features)\n",
        "\n",
        "    return(all_features)\n",
        "\n",
        "def compile_feature_data(data, label):\n",
        "    extracted_features = []\n",
        "    for curr_img_data in data:\n",
        "        for data_sample in curr_img_data:\n",
        "            data_sample_with_label = data_sample\n",
        "            data_sample_with_label.append(label)\n",
        "            extracted_features.append(data_sample_with_label)\n",
        "    return(extracted_features)\n",
        "\n",
        "def compile_train_data(feature_data_blur, feature_data_sharp, feature_data_motion_blur):\n",
        "    extracted_features_sharp = compile_feature_data(feature_data_sharp, label=1)\n",
        "    extracted_features_blur = compile_feature_data(feature_data_blur, label=0)\n",
        "    extracted_features_motion_blur = compile_feature_data(feature_data_motion_blur, label=0)\n",
        "    train_data = np.concatenate((extracted_features_sharp, extracted_features_blur, extracted_features_motion_blur), axis=0)\n",
        "    return(train_data)\n",
        "\n",
        "def start_training(train_data, batch_size, num_epochs, save_model=False):\n",
        "    train_data_loader = DataLoader(TrainDataset(train_data), batch_size=batch_size, shuffle=True)\n",
        "    data_dim = np.shape(train_data)[1]-1\n",
        "    model = MLP(data_dim).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        losses = []\n",
        "        for batch_num, input_data in enumerate(train_data_loader):\n",
        "            optimizer.zero_grad()\n",
        "            x, y = input_data\n",
        "            x = x.to(device).float()\n",
        "            y = y.to(device)\n",
        "\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y.flatten().long().to(device))\n",
        "            loss.backward()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        print('Epoch %d | Loss %6.2f' % (epoch, sum(losses) / len(losses)))\n",
        "    state_dict = {'model_state': model}\n",
        "    if(save_model):\n",
        "        torch.save(state_dict, './trained_model/trained_model')\n",
        "    return(model)\n",
        "\n",
        "def compute_train_accuracy(trained_model, train_data):\n",
        "    train_loader = DataLoader(TrainDataset(train_data), batch_size=batch_size, shuffle=True)\n",
        "    total_num_samples = 0\n",
        "    correct_prediction = 0\n",
        "    for batch_num, input_data in enumerate(train_loader):\n",
        "        x, y = input_data\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = trained_model(x)\n",
        "        _, predicted_label = torch.max(output, 1)\n",
        "\n",
        "        correct_prediction += (predicted_label == y).sum().item()\n",
        "        total_num_samples += output.shape[0]\n",
        "    accuracy = correct_prediction / total_num_samples\n",
        "    print('Train Accuracy = ')\n",
        "    print(accuracy)\n",
        "    return(accuracy)\n",
        "\n",
        "def balance_data(X_Train, Y_Train):\n",
        "    # Shuffle the samples\n",
        "    index_arr = [i for i in range(len(Y_Train))]\n",
        "    random.shuffle(index_arr)\n",
        "    X_shuffled = X_Train[index_arr, :]\n",
        "    Y_shuffled = Y_Train[index_arr]\n",
        "\n",
        "    # Seperate the samples in positive and negative bin\n",
        "    positive_idx = np.where(Y_shuffled == 1)\n",
        "    negative_idx = np.where(Y_shuffled == 0)\n",
        "    X_positive = X_shuffled[positive_idx]\n",
        "    X_negative = X_shuffled[negative_idx]\n",
        "    Y_positive = Y_shuffled[positive_idx]\n",
        "    Y_negative = Y_shuffled[negative_idx]\n",
        "\n",
        "    num_positive_samples = len(np.argwhere(Y_shuffled == 1))\n",
        "    num_negative_samples = len(np.argwhere(Y_shuffled == 0))\n",
        "\n",
        "    if(num_positive_samples < num_negative_samples):\n",
        "        selected_X_negative = X_negative[0:num_positive_samples, :]\n",
        "        selected_Y_negative = Y_negative[0:num_positive_samples]\n",
        "        selected_X_positive = X_positive\n",
        "        selected_Y_positive = Y_positive\n",
        "    else:\n",
        "        selected_X_negative = X_negative\n",
        "        selected_Y_negative = Y_negative\n",
        "        selected_X_positive = X_positive[0:num_positive_samples, :]\n",
        "        selected_Y_positive = Y_positive[0:num_positive_samples]\n",
        "\n",
        "    X_Train = np.concatenate((selected_X_positive, selected_X_negative), axis=0)\n",
        "    Y_Train = np.concatenate((selected_Y_positive, selected_Y_negative), axis=0)\n",
        "\n",
        "    train_data = []\n",
        "    for i in range(len(Y_Train)):\n",
        "        train_data.append(np.concatenate((X_Train[i], [Y_Train[i]])))\n",
        "\n",
        "    return(train_data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_dir = './dataset/defocused_blurred/'\n",
        "    feature_data_blur = extract_feature_data(dataset_dir)\n",
        "\n",
        "    dataset_dir = './dataset/motion_blurred/'\n",
        "    feature_data_motion_blur = extract_feature_data(dataset_dir)\n",
        "\n",
        "    dataset_dir = './dataset/sharp/'\n",
        "    feature_data_sharp = extract_feature_data(dataset_dir)\n",
        "\n",
        "    train_data = compile_train_data(feature_data_blur, feature_data_sharp, feature_data_motion_blur)\n",
        "\n",
        "    # Balance the data\n",
        "    dim = np.shape(train_data)[1]-1\n",
        "    X_Train = train_data[:, 0:dim]\n",
        "    Y_Train = train_data[:, -1]\n",
        "\n",
        "    train_data = balance_data(X_Train, Y_Train)\n",
        "    # Start the training\n",
        "    batch_size = 1024\n",
        "    num_epochs = 50\n",
        "\n",
        "    trained_model = start_training(train_data, batch_size, num_epochs, save_model=True)\n",
        "\n",
        "    accuracy = compute_train_accuracy(trained_model, train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "duJOzZbygTen",
        "outputId": "2cab04a5-cb06-4e81-fc0d-2eed2a116bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    }
  ]
}